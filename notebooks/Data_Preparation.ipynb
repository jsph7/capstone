{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Data Preparation]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Short ideas data from Seeking Alpha\n",
    "\n",
    "Seeking Alpha short selling article data was collected by web scraping, using Scrapy. The scraper ran on an Amazon Web Services (AWS) EC2 instance, and the results were stored in CSV format in an S3 bucket. There was significant web scraping issue because there is a strict limitation on the amount of data that can be scraped at once from SA. This was resolved by using Amazon EC2 and tweaking user-agent setting on Scrapy. \n",
    "\n",
    "Seeking Alpha web pages are pretty well structured therefore the following fields can be obtained after a little bit of processing.\n",
    "* article id, publication date, editors' pick info, url\n",
    "* author id, name and url\n",
    "* company name and ticker\n",
    "\n",
    "Scrapy codes for the web scraping can be found here and the data processing codes are shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries for data construction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To plot figures on jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Set max rows and columns to display\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate web scraped csv files\n",
    "Short ideas data is scraped and save in many csv files. Let's concatenate them and create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate web-scraped csv files \n",
    "import glob, os\n",
    "\n",
    "path = '/Users/Woosik/capstone_my/data/list'\n",
    "all_files = glob.glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f, encoding = 'ISO-8859-1') for f in all_files)\n",
    "df = pd.concat(df_from_each_file, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataframe and create variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean variable name and drop duplicates\n",
    "df.rename(columns={'info':'article_info'}, inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Some articles have no ticker information. Usually those articles discuss overall sector or market.\n",
    "# Let's ignore (delete) those articles for this project.\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Clean author_name column\n",
    "df['author_name'] = df.apply(lambda x: x.author_name.replace(x.ticker+',', ''), axis=1)\n",
    "\n",
    "# Extract datetime from article_info column\n",
    "import re\n",
    "from datetime import datetime\n",
    "df['article_info'] = [i.replace('May', 'May.') for i in df.article_info]\n",
    "df['date_temp'] = [re.split('\\.|M,', i)[0][-3:] + '.' + re.split('\\.|M,', i)[1] + 'M' for i in df.article_info]\n",
    "df['date'] = pd.to_datetime(df['date_temp'], format='%b. %d, %Y, %I:%M %p') \n",
    "\n",
    "# Extract Editors' pick info as boolean\n",
    "df['editors_pick'] = df['article_info'].str.split(',').str[0] == \"Editors' Pick\"\n",
    "\n",
    "# Create a variable indicating the number of stocks discussed in the article\n",
    "df['num_ticker'] = df.ticker.str.count(',') + 1\n",
    "\n",
    "# Create full url for later scraping\n",
    "df['author_url'] = 'https://seekingalpha.com' + df['author_link']\n",
    "df['article_url'] = 'https://seekingalpha.com' + df['article_link']\n",
    "\n",
    "# Sort dataframe and reset index\n",
    "df.sort_values(by=['date'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stocks discussed in the article\n",
    "\n",
    "SA assigns a unique id to each article. In addition, SA editors tag each article with one or more stock tickers prior to publication. Single-ticker articles focus solely on one stock, making it relatively easy to extract the author’s opinion on that company. Multiple-ticker articles discuss more than one stock in the same article, rendering extraction of the author’s various opinions for each of the tagged stocks difficult, if not impossible. I therefore focus my analysis on single-ticker articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14003\n",
       "2      350\n",
       "3      253\n",
       "Name: num_ticker, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.num_ticker.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep single-ticker articles only for this project.\n",
    "df = df[df['num_ticker'] == 1]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Keep relevant columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[['date','editors_pick','article_id','author_id','author_name',\n",
    "         'company_name','ticker','num_ticker','article_link','author_link',\n",
    "         'article_url','author_url','article_info']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('data/df_single_ticker_articles.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SA opinion leaders data from Wayback Machine\n",
    "\n",
    "The list of opinion leaders of short ideas for every quarters during sample time period is manually collected from Wayback Machine (http://web.archive.org) and saved in excel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opinion leader data from Wayback Machine\n",
    "dfl = pd.read_excel('data/opinion_leader_list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>leader_2012Q1</th>\n",
       "      <th>leader_2012Q2</th>\n",
       "      <th>leader_2012Q3</th>\n",
       "      <th>leader_2012Q4</th>\n",
       "      <th>leader_2013Q1</th>\n",
       "      <th>leader_2013Q2</th>\n",
       "      <th>leader_2013Q3</th>\n",
       "      <th>leader_2013Q4</th>\n",
       "      <th>leader_2014Q1</th>\n",
       "      <th>leader_2014Q2</th>\n",
       "      <th>leader_2014Q3</th>\n",
       "      <th>leader_2014Q4</th>\n",
       "      <th>leader_2015Q1</th>\n",
       "      <th>leader_2015Q2</th>\n",
       "      <th>leader_2015Q3</th>\n",
       "      <th>leader_2015Q4</th>\n",
       "      <th>leader_2016Q1</th>\n",
       "      <th>leader_2016Q2</th>\n",
       "      <th>leader_2016Q3</th>\n",
       "      <th>leader_2016Q4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Bidness Etc</td>\n",
       "      <td>Bidness Etc</td>\n",
       "      <td>Ashraf Eassa</td>\n",
       "      <td>Core Equity Research</td>\n",
       "      <td>Kofi Bofah</td>\n",
       "      <td>Quoth the Raven</td>\n",
       "      <td>Quoth the Raven</td>\n",
       "      <td>Quoth the Raven</td>\n",
       "      <td>Quoth the Raven</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Anton Wahlman</td>\n",
       "      <td>Shock Exchange</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Anton Wahlman</td>\n",
       "      <td>Montana Skeptic</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Paulo Santos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>David White</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Ashraf Eassa</td>\n",
       "      <td>Ashraf Eassa</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Michael Blair</td>\n",
       "      <td>Michael Blair</td>\n",
       "      <td>Michael Blair</td>\n",
       "      <td>Michael Blair</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Anton Wahlman</td>\n",
       "      <td>Shock Exchange</td>\n",
       "      <td>Montana Skeptic</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Montana Skeptic</td>\n",
       "      <td>Montana Skeptic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rocco Pendola</td>\n",
       "      <td>Gutone</td>\n",
       "      <td>Saibus Research</td>\n",
       "      <td>Saibus Research</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Markos Kaminis</td>\n",
       "      <td>Michael Blair</td>\n",
       "      <td>Michael Blair</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Don Dion</td>\n",
       "      <td>Shock Exchange</td>\n",
       "      <td>Anton Wahlman</td>\n",
       "      <td>Michael Blair</td>\n",
       "      <td>Anton Wahlman</td>\n",
       "      <td>Shock Exchange</td>\n",
       "      <td>Josh Arnold</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Shock Exchange</td>\n",
       "      <td>Bill Maurer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Shmulik Karpf</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Shane Blackmon</td>\n",
       "      <td>Peter Pham</td>\n",
       "      <td>Dana Blankenhorn</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Don Dion</td>\n",
       "      <td>EnhydrisPECorp</td>\n",
       "      <td>The Pump Stopper</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>The Specialist</td>\n",
       "      <td>Brian Nichols</td>\n",
       "      <td>Whitney Tilson</td>\n",
       "      <td>Short/Long Trader</td>\n",
       "      <td>Brandon Dempster</td>\n",
       "      <td>Michael Blair</td>\n",
       "      <td>Anton Wahlman</td>\n",
       "      <td>Anton Wahlman</td>\n",
       "      <td>Shock Exchange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Takeover Analyst</td>\n",
       "      <td>Richard Saintvilus</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Modernist</td>\n",
       "      <td>David White</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>Richard Pearson</td>\n",
       "      <td>Seeking Profits</td>\n",
       "      <td>Seeking Profits</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Don Dion</td>\n",
       "      <td>Shock Exchange</td>\n",
       "      <td>Matt Stewart</td>\n",
       "      <td>Logical Thought</td>\n",
       "      <td>Orange Peel Investments</td>\n",
       "      <td>Anton Wahlman</td>\n",
       "      <td>Shock Exchange</td>\n",
       "      <td>Mark Hibben</td>\n",
       "      <td>Orange Peel Investments</td>\n",
       "      <td>Orange Peel Investments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank     leader_2012Q1       leader_2012Q2    leader_2012Q3  \\\n",
       "0     1      Paulo Santos        Paulo Santos      Bidness Etc   \n",
       "1     2       Bill Maurer         David White      Bill Maurer   \n",
       "2     3     Rocco Pendola              Gutone  Saibus Research   \n",
       "3     4     Shmulik Karpf         Bill Maurer   Shane Blackmon   \n",
       "4     5  Takeover Analyst  Richard Saintvilus     Paulo Santos   \n",
       "\n",
       "     leader_2012Q4     leader_2013Q1         leader_2013Q2    leader_2013Q3  \\\n",
       "0      Bidness Etc      Ashraf Eassa  Core Equity Research       Kofi Bofah   \n",
       "1      Bill Maurer       Bill Maurer          Ashraf Eassa     Ashraf Eassa   \n",
       "2  Saibus Research      Paulo Santos        Markos Kaminis    Michael Blair   \n",
       "3       Peter Pham  Dana Blankenhorn           Bill Maurer      Bill Maurer   \n",
       "4        Modernist       David White          Paulo Santos  Richard Pearson   \n",
       "\n",
       "     leader_2013Q4    leader_2014Q1     leader_2014Q2    leader_2014Q3  \\\n",
       "0  Quoth the Raven  Quoth the Raven   Quoth the Raven  Quoth the Raven   \n",
       "1      Bill Maurer    Michael Blair     Michael Blair    Michael Blair   \n",
       "2    Michael Blair      Bill Maurer          Don Dion   Shock Exchange   \n",
       "3         Don Dion   EnhydrisPECorp  The Pump Stopper     Paulo Santos   \n",
       "4  Seeking Profits  Seeking Profits       Bill Maurer         Don Dion   \n",
       "\n",
       "    leader_2014Q4  leader_2015Q1    leader_2015Q2            leader_2015Q3  \\\n",
       "0    Paulo Santos  Anton Wahlman   Shock Exchange             Paulo Santos   \n",
       "1   Michael Blair   Paulo Santos     Paulo Santos            Anton Wahlman   \n",
       "2   Anton Wahlman  Michael Blair    Anton Wahlman           Shock Exchange   \n",
       "3  The Specialist  Brian Nichols   Whitney Tilson        Short/Long Trader   \n",
       "4  Shock Exchange   Matt Stewart  Logical Thought  Orange Peel Investments   \n",
       "\n",
       "      leader_2015Q4    leader_2016Q1    leader_2016Q2  \\\n",
       "0      Paulo Santos    Anton Wahlman  Montana Skeptic   \n",
       "1    Shock Exchange  Montana Skeptic     Paulo Santos   \n",
       "2       Josh Arnold     Paulo Santos      Bill Maurer   \n",
       "3  Brandon Dempster    Michael Blair    Anton Wahlman   \n",
       "4     Anton Wahlman   Shock Exchange      Mark Hibben   \n",
       "\n",
       "             leader_2016Q3            leader_2016Q4  \n",
       "0             Paulo Santos             Paulo Santos  \n",
       "1          Montana Skeptic          Montana Skeptic  \n",
       "2           Shock Exchange              Bill Maurer  \n",
       "3            Anton Wahlman           Shock Exchange  \n",
       "4  Orange Peel Investments  Orange Peel Investments  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Daily stock data from CRSP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty columns for the stock related variables in the short ideas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the short ideas dataframe\n",
    "df = pd.read_pickle('data/df_single_ticker_articles.pkl')\n",
    "\n",
    "# Create empty columns\n",
    "df['5d_ret'] = np.nan  # 5 days (1 week)\n",
    "df['10d_ret'] = np.nan  # 10 days (2 weeks)\n",
    "df['20d_ret'] = np.nan  # 20 days (1 month)\n",
    "df['40d_ret'] = np.nan  # 40 days (2 months)\n",
    "df['60d_ret'] = np.nan  # 60 days (3 months)\n",
    "df['80d_ret'] = np.nan  # 80 days (4 months)\n",
    "df['100d_ret'] = np.nan  # 100 days (5 months)\n",
    "df['mkt_cap'] = np.nan\n",
    "df['siccd'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create start and end date variables to slice the dataset from CRSP and Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create start and end date variables to slice CRSP dataset\n",
    "from datetime import timedelta\n",
    "df['start_date'] = df['date'].dt.date\n",
    "df['end_date'] = df['start_date'] + timedelta(days=150)  # little bit longer than 5 months\n",
    "\n",
    "# Convert dates to string so that fix_yahoo_finance can recognize them\n",
    "def date_to_string(x):\n",
    "    try:\n",
    "        return datetime.strftime(x, '%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "df['start_date'] = df['start_date'].apply(date_to_string)\n",
    "df['end_date'] = df['end_date'].apply(date_to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Write functions that extract necessary information from CRSP daily stock dataset\n",
    "\n",
    "Daily stock data including ticker, return, price, number of shares outstanding and sic codes is obtained from CRSP through wrds (http://wrds-web.wharton.upenn.edu) and saved in seperate csv file by year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_generator(csv_file_name):\n",
    "    \n",
    "    \"\"\"Read CRSP stock daily csv file (already downloaded), clean and return dataframe.\n",
    "\n",
    "    Args:\n",
    "        csv_file_name: string of csv file name including '.csv'  ex) 'crsp_2012.csv'\n",
    "    Returns:\n",
    "        dataframe containing stock info such as date, ticker, return, mkt_cap and sic code. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create dictionary that contains dataframe for each year\n",
    "    df = pd.read_csv('data/CRSP/'+csv_file_name)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.dropna(subset=['ticker'], inplace=True)\n",
    "    \n",
    "    # Change type of date, from string to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'].astype(str), format='%Y%m%d')\n",
    "    \n",
    "    # Clean price data following the variable instrction from wrds\n",
    "    df['prc'] = [prc*-1 if prc < 0 else prc for prc in df['prc']]\n",
    "    df['prc'] = [np.nan if prc == 0 else prc for prc in df['prc']]\n",
    "    \n",
    "    # Calculate stock market cap (stock size)\n",
    "    df['mkt_cap'] = df.prc * df.shrout\n",
    "    \n",
    "    # Keep necessary variables only\n",
    "    df = df[['date', 'ticker', 'ret', 'mkt_cap', 'siccd']]\n",
    "    \n",
    "    # Reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quote_generator(df, ticker, start_date, end_date):\n",
    "    \n",
    "    \"\"\"Slice dataframe for specified ticker and time period.\n",
    "\n",
    "    Args:\n",
    "        df: dataframe with stock info\n",
    "        ticker: ticker of stock\n",
    "        start_date: start date of quote\n",
    "        end_date: end_date of quote\n",
    "    Returns:\n",
    "        quote containing stock info for specified ticker and time period. \n",
    "        \n",
    "    \"\"\"\n",
    "    return df[(df['ticker']==ticker) & ((df['date'] > start_date) & (df['date'] <= end_date))]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the empty cells of stock related variables in the short ideas dataframe\n",
    "In this project, the holding period returns from 5 business days (1 week) to 100 business days (5 months) will be used for the short portfolio performance analysis. This is because, due to the stock borrowing fee, it is hard to imagine that short sellers hold their short position longer than 5 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate compound returns of the stocks recommended in short ideas articles in year 2012~2016 (index: 0 to 12338)\n",
    "for i in range(12339):\n",
    "    yr = df.iloc[[i]].date.dt.year.astype(str)[i]\n",
    "    print('currently on line: ' + str(i))\n",
    "    try:\n",
    "        quotes = quote_generator(df_generator('crsp_'+yr+'.csv'), df.ticker[i], df.start_date[i], df.end_date[i])\n",
    "        quotes.reset_index(drop=True, inplace=True)\n",
    "        quotes['cum_ret'] = (1 + quotes.ret.astype(float)).cumprod() - 1\n",
    "        df['5d_ret'][i] = quotes.iloc[5,5]  # 5 days (1 week)\n",
    "        df['10d_ret'][i] = quotes.iloc[10,5]  # 10 days (2 weeks)\n",
    "        df['20d_ret'][i] = quotes.iloc[20,5]  # 20 days (1 month)\n",
    "        df['40d_ret'][i] = quotes.iloc[40,5]  # 40 days (2 months)\n",
    "        df['60d_ret'][i] = quotes.iloc[60,5]  # 60 days (3 months)\n",
    "        df['80d_ret'][i] = quotes.iloc[80,5]  # 80 days (4 months)\n",
    "        df['100d_ret'][i] = quotes.iloc[100,5]  # 100 days (5 months)\n",
    "        df['mkt_cap'][i] = quotes.iloc[0,3]  # adjusted prc at start date (or the closest business day)\n",
    "        df['siccd'][i] = quotes.iloc[0,4]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Save\n",
    "df.to_pickle('data/df_yahoo_crsp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and create variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/df_yahoo_crsp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>editors_pick</th>\n",
       "      <th>article_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>company_name</th>\n",
       "      <th>ticker</th>\n",
       "      <th>num_ticker</th>\n",
       "      <th>article_link</th>\n",
       "      <th>author_link</th>\n",
       "      <th>article_url</th>\n",
       "      <th>author_url</th>\n",
       "      <th>article_info</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_prc</th>\n",
       "      <th>5d_prc</th>\n",
       "      <th>10d_prc</th>\n",
       "      <th>20d_prc</th>\n",
       "      <th>40d_prc</th>\n",
       "      <th>60d_prc</th>\n",
       "      <th>80d_prc</th>\n",
       "      <th>100d_prc</th>\n",
       "      <th>5d_ret</th>\n",
       "      <th>10d_ret</th>\n",
       "      <th>20d_ret</th>\n",
       "      <th>40d_ret</th>\n",
       "      <th>60d_ret</th>\n",
       "      <th>80d_ret</th>\n",
       "      <th>100d_ret</th>\n",
       "      <th>mkt_cap</th>\n",
       "      <th>siccd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-01 11:18:00</td>\n",
       "      <td>False</td>\n",
       "      <td>316904</td>\n",
       "      <td>973944</td>\n",
       "      <td>Honne Capital</td>\n",
       "      <td>Mattress Firm Holding Corp.</td>\n",
       "      <td>MFRM</td>\n",
       "      <td>1</td>\n",
       "      <td>/article/316904-mattress-firm-should-provide-m...</td>\n",
       "      <td>/author/honne-capital/articles</td>\n",
       "      <td>https://seekingalpha.com/article/316904-mattre...</td>\n",
       "      <td>https://seekingalpha.com/author/honne-capital/...</td>\n",
       "      <td>Û¢,Jan. 1, 2012, 11:18 AM,Û¢,Û¢,11åÊComments</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>2012-05-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034929</td>\n",
       "      <td>0.168177</td>\n",
       "      <td>0.374773</td>\n",
       "      <td>0.465286</td>\n",
       "      <td>0.643379</td>\n",
       "      <td>0.726604</td>\n",
       "      <td>0.670544</td>\n",
       "      <td>835782.75</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-02 05:56:00</td>\n",
       "      <td>False</td>\n",
       "      <td>316946</td>\n",
       "      <td>360252</td>\n",
       "      <td>The GeoTeam</td>\n",
       "      <td>Raystream Inc.</td>\n",
       "      <td>RAYS</td>\n",
       "      <td>1</td>\n",
       "      <td>/article/316946-raystream-remains-under-scruti...</td>\n",
       "      <td>/author/the-geoteam/articles</td>\n",
       "      <td>https://seekingalpha.com/article/316946-raystr...</td>\n",
       "      <td>https://seekingalpha.com/author/the-geoteam/ar...</td>\n",
       "      <td>Û¢,Jan. 2, 2012, 5:56 AM,Û¢,Û¢,26åÊComments</td>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>2012-05-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-03 09:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>317052</td>\n",
       "      <td>201452</td>\n",
       "      <td>Gary Weiss</td>\n",
       "      <td>Overstock.com, Inc.</td>\n",
       "      <td>OSTK</td>\n",
       "      <td>1</td>\n",
       "      <td>/article/317052-does-patrick-byrnes-overstock-...</td>\n",
       "      <td>/author/gary-weiss/articles</td>\n",
       "      <td>https://seekingalpha.com/article/317052-does-p...</td>\n",
       "      <td>https://seekingalpha.com/author/gary-weiss/art...</td>\n",
       "      <td>Û¢,Jan. 3, 2012, 9:00 AM,Û¢</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.089493</td>\n",
       "      <td>-0.115434</td>\n",
       "      <td>-0.105058</td>\n",
       "      <td>-0.207523</td>\n",
       "      <td>-0.320363</td>\n",
       "      <td>-0.217901</td>\n",
       "      <td>-0.141379</td>\n",
       "      <td>174825.29</td>\n",
       "      <td>5999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-04 14:40:00</td>\n",
       "      <td>False</td>\n",
       "      <td>317392</td>\n",
       "      <td>1072567</td>\n",
       "      <td>Ryan Canady</td>\n",
       "      <td>Lululemon Athletica Inc.</td>\n",
       "      <td>LULU</td>\n",
       "      <td>1</td>\n",
       "      <td>/article/317392-lululemon-is-a-sell-and-the-in...</td>\n",
       "      <td>/author/ryan-canady/articles</td>\n",
       "      <td>https://seekingalpha.com/article/317392-lulule...</td>\n",
       "      <td>https://seekingalpha.com/author/ryan-canady/ar...</td>\n",
       "      <td>Û¢,Jan. 4, 2012, 2:40 PM,Û¢,Û¢,80åÊComments</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>2012-06-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197263</td>\n",
       "      <td>0.175366</td>\n",
       "      <td>0.262560</td>\n",
       "      <td>0.345453</td>\n",
       "      <td>0.461190</td>\n",
       "      <td>0.486801</td>\n",
       "      <td>0.426781</td>\n",
       "      <td>5668688.40</td>\n",
       "      <td>5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-04 20:01:00</td>\n",
       "      <td>False</td>\n",
       "      <td>317485</td>\n",
       "      <td>1017993</td>\n",
       "      <td>Bill Maurer</td>\n",
       "      <td>Netflix, Inc.</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>1</td>\n",
       "      <td>/article/317485-is-it-possible-that-we-are-too...</td>\n",
       "      <td>/author/bill-maurer/articles</td>\n",
       "      <td>https://seekingalpha.com/article/317485-is-it-...</td>\n",
       "      <td>https://seekingalpha.com/author/bill-maurer/ar...</td>\n",
       "      <td>Û¢,Jan. 4, 2012, 8:01 PM,Û¢,Û¢,49åÊComments</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>2012-06-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.145431</td>\n",
       "      <td>0.245991</td>\n",
       "      <td>0.571534</td>\n",
       "      <td>0.377378</td>\n",
       "      <td>0.416656</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>-0.162958</td>\n",
       "      <td>4393140.70</td>\n",
       "      <td>7841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  editors_pick  article_id  author_id    author_name  \\\n",
       "0 2012-01-01 11:18:00         False      316904     973944  Honne Capital   \n",
       "1 2012-01-02 05:56:00         False      316946     360252    The GeoTeam   \n",
       "2 2012-01-03 09:00:00         False      317052     201452     Gary Weiss   \n",
       "3 2012-01-04 14:40:00         False      317392    1072567    Ryan Canady   \n",
       "4 2012-01-04 20:01:00         False      317485    1017993    Bill Maurer   \n",
       "\n",
       "                  company_name ticker  num_ticker  \\\n",
       "0  Mattress Firm Holding Corp.   MFRM           1   \n",
       "1               Raystream Inc.   RAYS           1   \n",
       "2          Overstock.com, Inc.   OSTK           1   \n",
       "3     Lululemon Athletica Inc.   LULU           1   \n",
       "4                Netflix, Inc.   NFLX           1   \n",
       "\n",
       "                                        article_link  \\\n",
       "0  /article/316904-mattress-firm-should-provide-m...   \n",
       "1  /article/316946-raystream-remains-under-scruti...   \n",
       "2  /article/317052-does-patrick-byrnes-overstock-...   \n",
       "3  /article/317392-lululemon-is-a-sell-and-the-in...   \n",
       "4  /article/317485-is-it-possible-that-we-are-too...   \n",
       "\n",
       "                      author_link  \\\n",
       "0  /author/honne-capital/articles   \n",
       "1    /author/the-geoteam/articles   \n",
       "2     /author/gary-weiss/articles   \n",
       "3    /author/ryan-canady/articles   \n",
       "4    /author/bill-maurer/articles   \n",
       "\n",
       "                                         article_url  \\\n",
       "0  https://seekingalpha.com/article/316904-mattre...   \n",
       "1  https://seekingalpha.com/article/316946-raystr...   \n",
       "2  https://seekingalpha.com/article/317052-does-p...   \n",
       "3  https://seekingalpha.com/article/317392-lulule...   \n",
       "4  https://seekingalpha.com/article/317485-is-it-...   \n",
       "\n",
       "                                          author_url  \\\n",
       "0  https://seekingalpha.com/author/honne-capital/...   \n",
       "1  https://seekingalpha.com/author/the-geoteam/ar...   \n",
       "2  https://seekingalpha.com/author/gary-weiss/art...   \n",
       "3  https://seekingalpha.com/author/ryan-canady/ar...   \n",
       "4  https://seekingalpha.com/author/bill-maurer/ar...   \n",
       "\n",
       "                                      article_info  start_date    end_date  \\\n",
       "0  Û¢,Jan. 1, 2012, 11:18 AM,Û¢,Û¢,11åÊComments  2012-01-01  2012-05-30   \n",
       "1   Û¢,Jan. 2, 2012, 5:56 AM,Û¢,Û¢,26åÊComments  2012-01-02  2012-05-31   \n",
       "2                    Û¢,Jan. 3, 2012, 9:00 AM,Û¢  2012-01-03  2012-06-01   \n",
       "3   Û¢,Jan. 4, 2012, 2:40 PM,Û¢,Û¢,80åÊComments  2012-01-04  2012-06-02   \n",
       "4   Û¢,Jan. 4, 2012, 8:01 PM,Û¢,Û¢,49åÊComments  2012-01-04  2012-06-02   \n",
       "\n",
       "   start_prc  5d_prc  10d_prc  20d_prc  40d_prc  60d_prc  80d_prc  100d_prc  \\\n",
       "0        NaN     NaN      NaN      NaN      NaN      NaN      NaN       NaN   \n",
       "1        NaN     NaN      NaN      NaN      NaN      NaN      NaN       NaN   \n",
       "2        NaN     NaN      NaN      NaN      NaN      NaN      NaN       NaN   \n",
       "3        NaN     NaN      NaN      NaN      NaN      NaN      NaN       NaN   \n",
       "4        NaN     NaN      NaN      NaN      NaN      NaN      NaN       NaN   \n",
       "\n",
       "     5d_ret   10d_ret   20d_ret   40d_ret   60d_ret   80d_ret  100d_ret  \\\n",
       "0  0.034929  0.168177  0.374773  0.465286  0.643379  0.726604  0.670544   \n",
       "1       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2 -0.089493 -0.115434 -0.105058 -0.207523 -0.320363 -0.217901 -0.141379   \n",
       "3  0.197263  0.175366  0.262560  0.345453  0.461190  0.486801  0.426781   \n",
       "4  0.145431  0.245991  0.571534  0.377378  0.416656  0.011312 -0.162958   \n",
       "\n",
       "      mkt_cap siccd  \n",
       "0   835782.75  9999  \n",
       "1         NaN   NaN  \n",
       "2   174825.29  5999  \n",
       "3  5668688.40  5600  \n",
       "4  4393140.70  7841  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary variables\n",
    "df.drop(['start_prc','5d_prc','10d_prc','20d_prc','40d_prc','60d_prc','80d_prc','100d_prc'], axis=1, inplace=True)\n",
    "df.drop(df[df.date.dt.year==2017].index, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Create date variables for later analysis\n",
    "df.rename(columns={'date': 'datetime'}, inplace=True)\n",
    "df['date'] = df['datetime'].dt.date\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['yr_qt'] = df['datetime'].dt.to_period(\"Q\")\n",
    "df['yr_qt_str'] = df.yr_qt.astype(str)\n",
    "\n",
    "# Manually drop rows containing erroneous return values\n",
    "df.drop(df[(df['5d_ret']>10)|(df['40d_ret']>9)].index, inplace=True)\n",
    "\n",
    "# Clean sic code\n",
    "df.loc[df.siccd=='Z', 'siccd'] = 0\n",
    "df.siccd = df.siccd.astype(int)\n",
    "df.siccd = [format(sic, '04d') for sic in df.siccd]\n",
    "df['sic2'] = df['siccd'].str[:2].astype(int)\n",
    "\n",
    "# Assign sic major industry classification\n",
    "df.loc[(df.sic2>=1)&(df.sic2<=9), 'sic2_name'] = 'Agriculture'\n",
    "df.loc[(df.sic2>=10)&(df.sic2<=14), 'sic2_name'] = 'Mining'\n",
    "df.loc[(df.sic2>=15)&(df.sic2<=17), 'sic2_name'] = 'Construction'\n",
    "df.loc[(df.sic2>=20)&(df.sic2<=39), 'sic2_name'] = 'Manufacturing'\n",
    "df.loc[(df.sic2>=40)&(df.sic2<=49), 'sic2_name'] = 'Transportation'\n",
    "df.loc[(df.sic2>=50)&(df.sic2<=51), 'sic2_name'] = 'Wholesale'\n",
    "df.loc[(df.sic2>=52)&(df.sic2<=59), 'sic2_name'] = 'Retail'\n",
    "df.loc[(df.sic2>=60)&(df.sic2<=67), 'sic2_name'] = 'Finance'\n",
    "df.loc[(df.sic2>=70)&(df.sic2<=89), 'sic2_name'] = 'Services'\n",
    "df.loc[(df.sic2>=90)&(df.sic2<=98), 'sic2_name'] = 'Public Admin'\n",
    "df.loc[df.sic2==99, 'sic2_name'] = 'Nonclassifiable'\n",
    "df.loc[df.sic2==0, 'sic2_name'] = 'N/A'\n",
    "\n",
    "# Change sic industry category for later analysis\n",
    "df.loc[df.sic2_name == 'Transportation', 'sic2_name'] = 'Telecom'\n",
    "df.loc[df.company_name == 'Tesla, Inc.', 'sic2_name'] = 'Manufacturing'\n",
    "df.loc[df.company_name == 'Facebook', 'sic2_name'] = 'Services'\n",
    "df.loc[df.company_name == 'Groupon, Inc.', 'sic2_name'] = 'Services'\n",
    "df.loc[df.company_name == 'GoPro', 'sic2_name'] = 'Manufacturing'\n",
    "df.loc[df.company_name == 'ANGI Homeservices Inc.', 'sic2_name'] = 'Services'\n",
    "df.loc[df.company_name == 'Zynga', 'sic2_name'] = 'Services'\n",
    "df.loc[df.company_name == 'Zillow Group, Inc.', 'sic2_name'] = 'Services'\n",
    "df.loc[df.ticker == 'OMEX', 'sic2_name'] = 'Mining'\n",
    "\n",
    "df.loc[(df.sic2_name!='Manufacturing')&(df.sic2_name!='Services')&(df.sic2_name!='Retail')&\n",
    "       (df.sic2_name!='Telecom')&(df.sic2_name!='Finance')&(df.sic2_name!='Mining')&\n",
    "       (df.sic2_name!='Wholesale')&(df.sic2_name!='Nonclassifiable'), 'sic2_name'] = 'Etc.'\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge the dataframe with disclosure information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataframe containing the disclosure information and article body\n",
    "df_nlp = pd.read_pickle('data/df_all_VADER.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct categorical variable about disclosure \n",
    "# 1. short/ 2. long/ 3. no position- no plan/ 4. no position- may initiate a short position/ 5. no info)\n",
    "df_nlp['body_str'] = df_nlp['body'].apply(lambda x: ' '.join(map(str, x)))\n",
    "conditions = [\n",
    "    (df_nlp.disclosure.notnull()&df_nlp.disclosure.str.contains('are short|am short|is short')) \n",
    "    | (df_nlp.disclosure.isnull()&df_nlp.body_str.str.contains('are short|am short|is short')), # 1. short\n",
    "    (df_nlp.disclosure.notnull()&df_nlp.disclosure.str.contains('are long|am long|is long')) \n",
    "    | (df_nlp.disclosure.isnull()&df_nlp.body_str.str.contains('are long|am long|is long')), # 2. long\n",
    "    (df_nlp.disclosure.notnull()&df_nlp.disclosure.str.contains('no positions in any stocks mentioned, and no plans')) \n",
    "    | (df_nlp.disclosure.isnull()&df_nlp.body_str.str.contains('no positions in any stocks mentioned, and no plans')), # 3. no position- no plan\n",
    "    (df_nlp.disclosure.notnull()&df_nlp.disclosure.str.contains('no positions in any stocks mentioned, but may initiate')) \n",
    "    | (df_nlp.disclosure.isnull()&df_nlp.body_str.str.contains('no positions in any stocks mentioned, but may initiate'))] # 4. no position- may initiate\n",
    "choices = ['Short', 'Long', 'No Position No Plan', 'No Position But May']\n",
    "df_nlp['disclosure_cat'] = np.select(conditions, choices, default='n/a')\n",
    "df_nlp.drop('body_str', axis=1, inplace=True) \n",
    "\n",
    "# Merge df with df_nlp\n",
    "df_inner = df.merge(df_nlp, how='inner', on='article_id')\n",
    "\n",
    "# Keep necessary variables only\n",
    "df_inner = df_inner[['datetime', 'date_x', 'article_id', 'author_id_x', 'author_name_x', \n",
    "                     'editors_pick_x', 'company_name_x', 'ticker_x', 'article_url_x', 'author_url_x', \n",
    "                     '5d_ret', '10d_ret', '20d_ret', '40d_ret', '60d_ret', '80d_ret', '100d_ret',\n",
    "                     'mkt_cap', 'siccd', 'sic2', 'sic2_name', 'year', 'yr_qt', 'yr_qt_str',\n",
    "                     'author_bio', 'body', 'disclosure_cat', 'Num_syl', 'Num_words', 'Num_sent', \n",
    "                     'neg_ratio_abs', 'pos_ratio_abs', 'unc_ratio_abs', 'v_neg', 'v_pos', 'v_neu', 'v_comp']]\n",
    "\n",
    "# Change variables name\n",
    "df_inner.rename(columns={'date_x':'date', 'author_id_x':'author_id', 'author_name_x':'author_name',\n",
    "                         'editors_pick_x':'editors_pick', 'company_name_x':'company_name',\n",
    "                         'ticker_x':'ticker', 'article_url_x': 'article_url', \n",
    "                         'author_url_x':'author_url'}, inplace=True)\n",
    "\n",
    "# Keep data from 2012 to 2015\n",
    "df_inner = df_inner[df_inner.year!=2016]\n",
    "\n",
    "# Save\n",
    "df_inner.to_pickle('data/df_inner_nlp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7873 entries, 0 to 7872\n",
      "Data columns (total 37 columns):\n",
      "datetime          7873 non-null datetime64[ns]\n",
      "date              7873 non-null datetime64[ns]\n",
      "article_id        7873 non-null int64\n",
      "author_id         7873 non-null int64\n",
      "author_name       7873 non-null object\n",
      "editors_pick      7873 non-null bool\n",
      "company_name      7873 non-null object\n",
      "ticker            7873 non-null object\n",
      "article_url       7873 non-null object\n",
      "author_url        7873 non-null object\n",
      "5d_ret            7873 non-null float64\n",
      "10d_ret           7873 non-null float64\n",
      "20d_ret           7873 non-null float64\n",
      "40d_ret           7873 non-null float64\n",
      "60d_ret           7873 non-null float64\n",
      "80d_ret           7873 non-null float64\n",
      "100d_ret          7873 non-null float64\n",
      "mkt_cap           7873 non-null float64\n",
      "siccd             7873 non-null object\n",
      "sic2              7873 non-null int64\n",
      "sic2_name         7873 non-null object\n",
      "year              7873 non-null int64\n",
      "yr_qt             7873 non-null object\n",
      "yr_qt_str         7873 non-null object\n",
      "author_bio        7089 non-null object\n",
      "body              7873 non-null object\n",
      "disclosure_cat    7873 non-null object\n",
      "Num_syl           7873 non-null int64\n",
      "Num_words         7873 non-null int64\n",
      "Num_sent          7873 non-null int64\n",
      "neg_ratio_abs     7873 non-null float64\n",
      "pos_ratio_abs     7873 non-null float64\n",
      "unc_ratio_abs     7873 non-null float64\n",
      "v_neg             7873 non-null float64\n",
      "v_pos             7873 non-null float64\n",
      "v_neu             7873 non-null float64\n",
      "v_comp            7873 non-null float64\n",
      "dtypes: bool(1), datetime64[ns](2), float64(15), int64(7), object(12)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_inner.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
